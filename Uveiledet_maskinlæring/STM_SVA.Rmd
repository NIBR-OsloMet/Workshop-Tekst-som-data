---
title: "STM_SVA"
author: "Yuri Kasahara"
date: "2024-03-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Structural Topic Model (STM)

Vi bruker samme logikk for å bygge opp en LDA model. Største fordel er at STM bibliotek har mange innbygde funksjoner i seg.

```{r, error=FALSE, warning=FALSE, message=FALSE, echo=TRUE, results='hide'}
library(tm)
library(topicmodels)
library(stm)
library(tidyverse)
library(quanteda)
library(stopwords)
library(tidytext)
```

```{r, echo=TRUE}
# Prossesering av tekst er en del enklere for bygge opp vår Document-Term Matrix

info_reports_SVA_with_lang <- read.csv("info_reports_SVA_with_lang.csv", encoding = "UTF-8")

info_reports_SVA_no <- info_reports_SVA_with_lang[c('Abstract', 'Institute', 'Year')]


extra_words <- c("the", "that", "and", "this", "are", "more", "has", "than",
                 "with", "also", "have", "has", "been", "from", "their", "derfor",
                 "eksempel")

info_reports.proc <- textProcessor(documents=info_reports_SVA_no$Abstract,
                                 metadata = info_reports_SVA_no,
                                 lowercase = TRUE, #*
                                 removestopwords = TRUE, #*
                                 removenumbers = TRUE, #*
                                 removepunctuation = TRUE, #*
                                 stem = TRUE, #*
                                 wordLengths = c(3,Inf), #*
                                 sparselevel = 1, #*
                                 language = "no", #*
                                 verbose = TRUE, #*
                                 onlycharacter = TRUE, # 
                                 striphtml = FALSE, #*
                                 customstopwords = extra_words, #*
                                 v1 = FALSE) #*


```

Nå trenger vi må prossesere korpora for å kjøre en STM.

```{r, echo=TRUE}

#Vi kan fjerne de mest/minst frekvente ordene basert på hvor ofte er de på dokumentene

processed_reports <- prepDocuments(info_reports.proc$documents, info_reports.proc$vocab, info_reports.proc$meta, lower.thresh = 2, upper.thresh = 100)

#docs <- processed_reports$documents
#vocab <- processed_reports$vocab
#meta <- processed_reports$meta




```

Vi kan se hvor mange dokumenter skal fjernes basert på forskjellige "tresholds".



I STM bibliotek har vi en funksjon som analysere forskjellige målstøkke for å prøve å finne den beste antall temaer for å modellere. Men huske å gi en spesifikasjon av hvordan meta-variabler påvirke temaer. Dette kan ta veldig lang tid!

```{r, echo=TRUE, results='hide'}

storage <- searchK(processed_reports$documents, processed_reports$vocab, K = c(2:15), prevalence=~Year+Institute, data = processed_reports$meta, heldout.seed = 1234)
```

Vi kan se resultatene

```{r}
plot.searchK(storage)
```

La oss kjøre en modell med 10 temaer

```{r}
InfoSVAPrevFit <- stm(processed_reports$documents, processed_reports$vocab, K=10, prevalence=~s(Year)+Institute, content =~ Institute, max.em.its=100, data=processed_reports$meta, init.type="Spectral", seed=1234)
```

Vi kan lage en figur for å vise hvor omfattende er hver tema

```{r}
plot(InfoSVAPrevFit, type="summary", xlim=c(0,.6))
```

La oss se hoved ordene av hvert temaet

```{r}
plot(InfoSVAPrevFit, type="labels", topics=c(1:10))
```

Nå kan vi estimere effekter av meta-variabler over temaer

```{r}
model <- estimateEffect(1:10 ~ Year + Institute, InfoSVAPrevFit, meta = processed_reports$meta, uncertainty = "Global")

summary(model, topics = c(1:5))

```

Vi kan lage figurer for å visualisere effekter av variabler over hver tema

```{r}
#For institute
plot.estimateEffect(model, covariate = "Institute", topics = c(3),
 model = InfoSVAPrevFit, method = "pointestimate",
 xlab = "Marginal topic proportion for each level",
 main = "Effect of Institute", xlim = c(-0.1, 0.6),
)

#For Year

plot(model, "Year", method = "continuous", topics = 3, printlegend = FALSE, xaxt = "n", xlab = "Year")

```

Vi kan se kort deler av dokumentene som mest identifiserte med et tema.

```{r}
short_abstract <- substr(meta$Abstract, start = 1, stop = 180)

example_docs <- findThoughts(InfoSVAPrevFit, texts = short_abstract, n=4, topics=3)

plotQuote(example_docs$docs[[1]])

```

Vi kan også se hvilken ord er mest knyttede til hvert institutt i hvert temaet.

```{r}

plot(InfoSVAPrevFit, type = "perspectives", covarlevels = c('AFI', 'SIFO'), topics = 3)

plot(InfoSVAPrevFit, type = "perspectives", covarlevels = c('NIBR', 'NOVA'), topics = 3)

```
